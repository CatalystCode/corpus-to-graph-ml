{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./packages\")\n",
    "\n",
    "import matplotlib\n",
    "import datetime\n",
    "import numpy as np\n",
    "import data_preparation_tools as dpt\n",
    "import features_generation_tools as fgt\n",
    "import model_tools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path used to save temporary doc2vec files\n",
    "temp_doc2vec_file = r\"\"\n",
    "# path to text file that contains background sentences used in doc2vec\n",
    "background_samples_file_path = r\"\"\n",
    "\n",
    "doc2vec_func = lambda x_train,x_test : fgt.get_doc2vec_features(x_train, x_test, temp_doc2vec_file, background_samples_file_path)\n",
    "bow_func = lambda x_train,x_test : fgt.get_bow_features(x_train, x_test, (1,3))\n",
    "\n",
    "# evaluate different features\n",
    "gen_features_methods = [\n",
    "fgt.GenFeaturesMethod(\"bow_1_gram\", lambda x_train,x_test : fgt.get_bow_features(x_train, x_test, (1,1))),\n",
    "fgt.GenFeaturesMethod(\"bow_2_gram\", lambda x_train,x_test : fgt.get_bow_features(x_train, x_test, (2,2))),\n",
    "fgt.GenFeaturesMethod(\"bow_3_gram\", lambda x_train,x_test : fgt.get_bow_features(x_train, x_test, (3,3))),\n",
    "fgt.GenFeaturesMethod(\"bow_1_3_gram\", lambda x_train,x_test : fgt.get_bow_features(x_train, x_test, (1,3))),\n",
    "fgt.GenFeaturesMethod(\"doc2vec\", lambda x_train,x_test : fgt.get_doc2vec_features(x_train, x_test, temp_doc2vec_file, background_samples_file_path))\n",
    "fgt.GenFeaturesMethod(\"pos_3_3\", lambda x_train,x_test : fgt.to_pos_bow(x_train, x_test, (3,3))),\n",
    "fgt.GenFeaturesMethod(\"bow_1_3_pos_3_3\", lambda x_train,x_test : fgt.get_bow_and_pos_features(x_train, x_test, (1,3), (3,3)))\n",
    "fgt.GenFeaturesMethod(\"bow_1_3_doc2vec\", lambda x_train,x_test : fgt.get_compound_features(x_train, x_test, [bow_func, doc2vec_func]))\n",
    "]\n",
    "\n",
    "#Cs= [0.005, 0.01, 0.03, 0.05, 0.1, 0.3, 0.5, 0.8] + np.linspace(1,5, 9).tolist()\n",
    "Cs = np.linspace(0.005,0.25,10)\n",
    "\n",
    "# evaluates different classifiers\n",
    "evaluation_methods = [\n",
    "    fgt.EvaluationMethod(\"logistic regression l1\", lambda: LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))\n",
    "    fgt.EvaluationMethod(\"lr l1 cv\", lambda: LogisticRegressionCV(penalty='l1', cv=5, scoring=make_scorer(f1_score), solver='liblinear', Cs=Cs, refit=True)),\n",
    "    fgt.EvaluationMethod(\"lr l2 cv\", lambda: LogisticRegressionCV(penalty='l2', cv=5, scoring=make_scorer(f1_score), solver='liblinear', Cs=Cs, refit=True))\n",
    "    fgt.EvaluationMethod(\"GBC\", lambda: GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=10, random_state=0))\n",
    "]\n",
    "\n",
    "# path to input dir \n",
    "input_dir = r\"\"\n",
    "startTime = datetime.datetime.now()\n",
    "\n",
    "models = fgt.run_gen_features_pipeline(input_dir, gen_features_methods, evaluation_methods)\n",
    "\n",
    "runTime = datetime.datetime.now() - startTime\n",
    "print \"Finished generating features, took:%s\"%runTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = models[0]\n",
    "\n",
    "av = [np.average(m.scores_[1][:,i]) for i in xrange(len(m.scores_[1][0])) ]\n",
    "plt.plot(Cs, av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# file path to data with test sentences\n",
    "test_file_path = r\"\"\n",
    "\n",
    "# path to context file path which contain the entities for each sentence as well (produced by the data transofrmation pipeline)\n",
    "context_file_path = r\"\"\n",
    "\n",
    "entities_1 = []\n",
    "entities_2 = []\n",
    "\n",
    "with open(context_file_path) as handle:\n",
    "    for l in handle:\n",
    "        parts = l.rstrip().split(\"\\t\")\n",
    "        entities_1.append(parts[0])\n",
    "        entities_2.append(parts[1])\n",
    "\n",
    "sentences, labels = fgt.read_data_from_file(test_file_path)\n",
    "\n",
    "import heapq\n",
    "import PrettyTable as pt\n",
    "\n",
    "COLUMN_NAMES = [\"score\", \"text\", \"Entity 1\", \"Entity2\"]\n",
    "\n",
    "er = models[0]\n",
    "\n",
    "neg_scores = [er.scores[i][0] for i in xrange(len(er.scores))]\n",
    "pos_scores = [er.scores[i][1] for i in xrange(len(er.scores))]\n",
    "\n",
    "\n",
    "def get_top_scoring(scores, sentences, entities_1, entities_2, n):\n",
    "    table_data = []\n",
    "    #scores_arr = np.array(scores)\n",
    "    #l = scores_arr.argsort()[-n:][::-1]\n",
    "    l = heapq.nlargest(n, range(len(scores)), scores.__getitem__)\n",
    "    for i in l:\n",
    "        table_data.append([\"%0.7f\"%scores[i], sentences[i], entities_1[i], entities_2[i]])\n",
    "    return table_data\n",
    "\n",
    "def write_results_to_file(file_path, column_names, table_data):\n",
    "    with open(file_path, \"w\") as handle:\n",
    "        handle.write(\"\\t\".join(COLUMN_NAMES) + \"\\n\")\n",
    "        for l in table_data:\n",
    "            handle.write(\"\\t\".join(l) + \"\\n\")\n",
    "\n",
    "# path to the file that will contain scored sentences, ordered by their score\n",
    "top_scoring_positive_class_file_path = r\"\"            \n",
    "\n",
    "write_results_to_file(top_scoring_positive_class_file_path, COLUMN_NAMES, get_top_scoring(pos_scores, sentences, entities_1, entities_2, len(pos_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
